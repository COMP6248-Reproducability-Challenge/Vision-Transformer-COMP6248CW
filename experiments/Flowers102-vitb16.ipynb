{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d6f28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:12.465556Z",
     "start_time": "2022-04-26T13:24:02.413507Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy\n",
    "\n",
    "from IPython.display import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from math import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f54a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:15.487097Z",
     "start_time": "2022-04-26T13:24:12.472196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set dataset and model\n",
    "\n",
    "# Dataset\n",
    "dataset = torchvision.datasets.Flowers102\n",
    "\n",
    "# Pre-trained models of ViT: \n",
    "# torchvision.models.vit_b_16/vit_b_32/vit_l_16/vit_l_32\n",
    "from torchvision.models import vit_b_16\n",
    "model = vit_b_16(pretrained=True)\n",
    "\n",
    "model_filename = \"flowers102-vitb16\"\n",
    "\n",
    "warmup_epoch = 10\n",
    "num_epoch = 30\n",
    "\n",
    "lr_warmup = 1e-3\n",
    "lr_base = 1e-2\n",
    "lr_min = 1e-3\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d2e033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:15.499925Z",
     "start_time": "2022-04-26T13:24:15.490215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d493b358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:16.402736Z",
     "start_time": "2022-04-26T13:24:15.502867Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224)\n",
    "])\n",
    "train_set = torchvision.datasets.Flowers102(\"./data\", split= \"train\", download = True, transform=transform)\n",
    "test_set = torchvision.datasets.Flowers102(\"./data\", split= \"test\", download = True, transform=transform)\n",
    "val_set = torchvision.datasets.Flowers102(\"./data\", split= \"val\", download = True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729470f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:16.412518Z",
     "start_time": "2022-04-26T13:24:16.408140Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035725cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:16.496148Z",
     "start_time": "2022-04-26T13:24:16.415905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = model.heads[0].in_features\n",
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502f062c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:16.553717Z",
     "start_time": "2022-04-26T13:24:16.499444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.heads = nn.Linear(in_features, 102) # set in_features and out_features\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d923adb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:24:16.625111Z",
     "start_time": "2022-04-26T13:24:16.557835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze class_token\n",
      "Freeze conv_proj.weight\n",
      "Freeze conv_proj.bias\n",
      "Freeze encoder.pos_embedding\n",
      "Freeze encoder.layers.encoder_layer_0.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_0.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_0.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_0.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_0.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_0.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_0.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_0.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_1.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_1.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_1.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_1.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_1.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_1.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_1.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_1.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_2.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_2.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_2.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_2.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_2.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_2.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_2.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_2.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_3.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_3.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_3.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_3.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_3.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_3.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_3.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_3.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_4.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_4.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_4.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_4.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_4.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_4.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_4.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_4.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_5.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_5.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_5.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_5.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_5.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_5.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_5.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_5.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_6.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_6.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_6.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_6.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_6.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_6.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_6.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_6.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_7.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_7.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_7.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_7.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_7.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_7.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_7.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_7.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_8.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_8.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_8.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_8.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_8.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_8.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_8.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_8.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_9.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_9.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_9.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_9.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_9.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_9.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_9.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_9.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_10.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_10.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_10.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_10.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_10.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_10.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_10.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_10.mlp.linear_2.bias\n",
      "Freeze encoder.layers.encoder_layer_11.ln_1.weight\n",
      "Freeze encoder.layers.encoder_layer_11.ln_1.bias\n",
      "Unfreeze encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "Unfreeze encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "Unfreeze encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "Unfreeze encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "Freeze encoder.layers.encoder_layer_11.ln_2.weight\n",
      "Freeze encoder.layers.encoder_layer_11.ln_2.bias\n",
      "Freeze encoder.layers.encoder_layer_11.mlp.linear_1.weight\n",
      "Freeze encoder.layers.encoder_layer_11.mlp.linear_1.bias\n",
      "Freeze encoder.layers.encoder_layer_11.mlp.linear_2.weight\n",
      "Freeze encoder.layers.encoder_layer_11.mlp.linear_2.bias\n",
      "Freeze encoder.ln.weight\n",
      "Freeze encoder.ln.bias\n",
      "Unfreeze heads.weight\n",
      "Unfreeze heads.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze other layers unless it is self_attention or mlp_head\n",
    "for name, param in model.named_parameters():\n",
    "    if ('self_attention' in name) or ('head' in name):\n",
    "        print(\"Unfreeze \" + name)\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        print(\"Freeze \" + name)\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3696240a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:28:43.939774Z",
     "start_time": "2022-04-26T13:24:16.630549Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading whole dataloader into memory can improve the speed of training\n",
    "train_loader = list(train_loader)\n",
    "test_loader = list(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e984877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:28:43.955198Z",
     "start_time": "2022-04-26T13:28:43.947354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Flowers102\n",
       "    Number of datapoints: 1020\n",
       "    Root location: ./data\n",
       "    split=train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506c0509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:28:44.010607Z",
     "start_time": "2022-04-26T13:28:43.957875Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e099b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:28:44.036575Z",
     "start_time": "2022-04-26T13:28:44.013055Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01579691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:28:47.405605Z",
     "start_time": "2022-04-26T13:28:44.038937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lr_base\n",
    "def adjust_learning_rate(optimizer, current_epoch, max_epoch, lr_min=lr_min, lr_max=lr_base, warmup=True):\n",
    "    if current_epoch < warmup_epoch:\n",
    "        lr = lr_max * (current_epoch+1) / (warmup_epoch+1)\n",
    "    else:\n",
    "        lr = lr_min + (lr_max-lr_min)*(1 + cos(pi * (current_epoch - warmup_epoch) / (max_epoch - warmup_epoch))) / 2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    print(\"Learning rate is set to \"+str(lr))\n",
    "\n",
    "optimiser = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                      lr=lr,\n",
    "                      momentum=0.9) #only optimse non-frozen layers\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef312b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:43.369518Z",
     "start_time": "2022-04-26T13:28:47.411544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is set to 0.0009090909090909091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07f29ea9e044f0588cf3ad43825861d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch0):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hf5n21/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/40:(tr)loss=74.4035\n",
      "epoch 0/40:(tr)acc=1.4706%\n",
      "Learning rate is set to 0.0018181818181818182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d099aa451886451fa283699e7c1efc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch1):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/40:(tr)loss=71.9159\n",
      "epoch 1/40:(tr)acc=4.5098%\n",
      "Learning rate is set to 0.002727272727272727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882fc4a3d4b74f1cb3ffa7bd0021e0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch2):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2/40:(tr)loss=66.3191\n",
      "epoch 2/40:(tr)acc=19.4118%\n",
      "Learning rate is set to 0.0036363636363636364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba240f3852436da542320d1b7f7c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch3):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3/40:(tr)loss=56.1171\n",
      "epoch 3/40:(tr)acc=53.3333%\n",
      "Learning rate is set to 0.004545454545454546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3025941e8d1944d784e38ec4362a74d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch4):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4/40:(tr)loss=41.2294\n",
      "epoch 4/40:(tr)acc=79.7059%\n",
      "Learning rate is set to 0.005454545454545454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa82e0667bc4d0f94f81bad285deb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch5):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/40:(tr)loss=25.6905\n",
      "epoch 5/40:(tr)acc=92.5490%\n",
      "Learning rate is set to 0.006363636363636364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a37433514a496fa41dfffe937b7edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch6):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6/40:(tr)loss=14.0369\n",
      "epoch 6/40:(tr)acc=97.7451%\n",
      "Learning rate is set to 0.007272727272727273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f22647eb034f2eba3f546a074312e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch7):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7/40:(tr)loss=7.3315\n",
      "epoch 7/40:(tr)acc=99.2157%\n",
      "Learning rate is set to 0.00818181818181818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a73dbab23af429baac4483fbb016245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch8):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8/40:(tr)loss=4.0013\n",
      "epoch 8/40:(tr)acc=99.8039%\n",
      "Learning rate is set to 0.009090909090909092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adebe0c89f3d484f9129a46c61a0b4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch9):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9/40:(tr)loss=2.3598\n",
      "epoch 9/40:(tr)acc=100.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf165505d320416bb93c9d95ff22549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test9:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hf5n21/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9/40:(te)loss=84.2538\n",
      "epoch 9/40:(te)acc=87.0548%\n",
      "Learning rate is set to 0.010000000000000002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7160474abf5d4aa2af9e569a3ec41b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch10):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10/40:(tr)loss=1.5797\n",
      "epoch 10/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.00997534852915723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f497ab05569047fd8050bb0fc8082ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch11):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11/40:(tr)loss=1.1446\n",
      "epoch 11/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.009901664203302126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd10dfe48a8b4e7f952ce378b1b1eb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch12):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12/40:(tr)loss=0.9097\n",
      "epoch 12/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.009779754323328192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8174eabe329a4964961278b5c05b17b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch13):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13/40:(tr)loss=0.7457\n",
      "epoch 13/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.009610954559391705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdce1b0a42d6446b9f061f297c77e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch14):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14/40:(tr)loss=0.6223\n",
      "epoch 14/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.009397114317029977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7887aab591f941e1b463bed82c9e8a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch15):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15/40:(tr)loss=0.5380\n",
      "epoch 15/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.009140576474687266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe4ea7282014e6e801908c9593f567b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch16):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16/40:(tr)loss=0.4796\n",
      "epoch 16/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.008844151714648276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9a25ea74b944feaea25d403e586a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch17):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17/40:(tr)loss=0.4342\n",
      "epoch 17/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.008511087728614863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38e28d879e847bba61a9dcedf88faa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch18):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18/40:(tr)loss=0.3985\n",
      "epoch 18/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.008145033635316129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e67acfb2f4848eab4665bc0fc970af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch19):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19/40:(tr)loss=0.3691\n",
      "epoch 19/40:(tr)acc=100.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adf5cec822145c494f6e3af5721f676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test19:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19/40:(te)loss=60.9277\n",
      "epoch 19/40:(te)acc=88.9413%\n",
      "Learning rate is set to 0.007750000000000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f3c2bdf4ec4cbbbd9130c393f3dfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch20):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/40:(tr)loss=0.3448\n",
      "epoch 20/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.007330314893841103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b293e038b6ab444d8fee582720c16bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch21):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21/40:(tr)loss=0.3245\n",
      "epoch 21/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.006890576474687264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56758e65a40348ce89d2e8de2e858377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch22):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22/40:(tr)loss=0.3074\n",
      "epoch 22/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.006435602608679917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16f5cd41fe34b8cb13d1502f2c35b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch23):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23/40:(tr)loss=0.2930\n",
      "epoch 23/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0059703780847044415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d5ade36d8143b096922351b7365516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch24):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24/40:(tr)loss=0.2806\n",
      "epoch 24/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.005500000000000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4d6ec656bf47f89ab42459e8309e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch25):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25/40:(tr)loss=0.2701\n",
      "epoch 25/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0050296219152955604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f77ca2e0f8b44eda800469dec6a7f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch26):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26/40:(tr)loss=0.2611\n",
      "epoch 26/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0045643973913200835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8ac7aa451a41cf8c5d304fcbae96ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch27):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27/40:(tr)loss=0.2534\n",
      "epoch 27/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.004109423525312737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beff45431a4b414b9afd6a634f7142d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch28):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28/40:(tr)loss=0.2468\n",
      "epoch 28/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0036696851061589\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c33b70562f34f8aaae4815de5b5033e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch29):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29/40:(tr)loss=0.2412\n",
      "epoch 29/40:(tr)acc=100.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dff9f68cc74b18a9e27809d807fd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test29:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29/40:(te)loss=57.4091\n",
      "epoch 29/40:(te)acc=89.1202%\n",
      "Learning rate is set to 0.003250000000000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612bd5e2dca442c9a20ada7a0dca6e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch30):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30/40:(tr)loss=0.2364\n",
      "epoch 30/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0028549663646838717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046a2c6ceff9468a9a15372fd2bc1708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch31):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31/40:(tr)loss=0.2323\n",
      "epoch 31/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0024889122713851394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd21de62b2d4c14ba470e75959673f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch32):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32/40:(tr)loss=0.2288\n",
      "epoch 32/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0021558482853517268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d862d96a2649a3b98308c1244f830b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch33):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33/40:(tr)loss=0.2259\n",
      "epoch 33/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0018594235253127371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737b354c14864cb2b325a18e3a45e649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch34):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34/40:(tr)loss=0.2234\n",
      "epoch 34/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0016028856829700259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fb51c422bc49cf86cc97bec7e650c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch35):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35/40:(tr)loss=0.2213\n",
      "epoch 35/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0013890454406082957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb81b8c905b4f72b63af3d018d4bbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch36):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36/40:(tr)loss=0.2195\n",
      "epoch 36/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.001220245676671809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61de1d05ffc5494c8036235adba6f77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch37):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37/40:(tr)loss=0.2180\n",
      "epoch 37/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0010983357966978745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e79786ce1324bc9abded299f7d879d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch38):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38/40:(tr)loss=0.2167\n",
      "epoch 38/40:(tr)acc=100.0000%\n",
      "Learning rate is set to 0.0010246514708427696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30a90e529c448a99df01a6e3ae647d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train(epoch39):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39/40:(tr)loss=0.2155\n",
      "epoch 39/40:(tr)acc=100.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3ea5be431f408b8ae3e051a801d2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test39:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39/40:(te)loss=56.5754\n",
      "epoch 39/40:(te)acc=89.1039%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for epoch in range(warmup_epoch+num_epoch):\n",
    "    running_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    adjust_learning_rate(optimizer=optimiser,\n",
    "                        current_epoch=epoch,\n",
    "                        max_epoch=warmup_epoch+num_epoch)\n",
    "    with tqdm(train_loader, desc='Train(epoch'+str(epoch)+')') as t:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in t:\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "            total += len(labels)\n",
    "            correct += sum(pred == labels)\n",
    "            \n",
    "        train_acc = (100.0 * correct) / total\n",
    "\n",
    "        t.set_postfix(running_loss=running_loss,\n",
    "                      runing_acc=train_acc)\n",
    "            \n",
    "    print(\"epoch %d/%d:(tr)loss=%.4f\" % (epoch, warmup_epoch+num_epoch, running_loss))\n",
    "    print(\"epoch %d/%d:(tr)acc=%.4f%%\" % (epoch, warmup_epoch+num_epoch, train_acc))\n",
    "    \n",
    "    test_running_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    if epoch%10==9:\n",
    "        \n",
    "        with tqdm(test_loader, desc='test'+str(epoch)) as t:\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for data in t:\n",
    "                    model.eval()\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    loss = loss_function(outputs, labels)\n",
    "                    test_running_loss += loss.item()\n",
    "\n",
    "                    pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "                    total += len(labels)\n",
    "                    correct += sum(pred == labels)\n",
    "                test_acc = (100.0 * correct) / total\n",
    "\n",
    "                t.set_postfix(running_loss=test_running_loss,\n",
    "                              runing_acc=test_acc)\n",
    "\n",
    "        print(\"epoch %d/%d:(te)loss=%.4f\" % (epoch, warmup_epoch+num_epoch, test_running_loss))\n",
    "        print(\"epoch %d/%d:(te)acc=%.4f%%\" % (epoch, warmup_epoch+num_epoch, test_acc))\n",
    "          \n",
    "    results.append({'running_loss':running_loss,\n",
    "                   'train_acc':train_acc,\n",
    "                   'test_running_loss':test_running_loss,\n",
    "                   'test_acc':test_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e06d91c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:43.445075Z",
     "start_time": "2022-04-26T13:40:43.373857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'running_loss': 74.40354537963867,\n",
       "  'train_acc': tensor(1.4706, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 71.9158844947815,\n",
       "  'train_acc': tensor(4.5098, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 66.31913661956787,\n",
       "  'train_acc': tensor(19.4118, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 56.11714959144592,\n",
       "  'train_acc': tensor(53.3333, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 41.22936463356018,\n",
       "  'train_acc': tensor(79.7059, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 25.690459728240967,\n",
       "  'train_acc': tensor(92.5490, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 14.036860942840576,\n",
       "  'train_acc': tensor(97.7451, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 7.331454664468765,\n",
       "  'train_acc': tensor(99.2157, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 4.001257449388504,\n",
       "  'train_acc': tensor(99.8039, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 2.3597824722528458,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 84.25381743907928,\n",
       "  'test_acc': tensor(87.0548, device='cuda:2')},\n",
       " {'running_loss': 1.579739362001419,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 1.1446147449314594,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.9096976183354855,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.7456599436700344,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.6222948618233204,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.5379923060536385,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.4795966651290655,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.4342442601919174,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.39846920408308506,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.36906508170068264,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 60.92765003442764,\n",
       "  'test_acc': tensor(88.9413, device='cuda:2')},\n",
       " {'running_loss': 0.34479532949626446,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.32451484352350235,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.30743039958178997,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.29295429587364197,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.2806316800415516,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.27011051308363676,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.2611106354743242,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.253406404517591,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.24681361857801676,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.24117885809391737,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 57.40906676650047,\n",
       "  'test_acc': tensor(89.1202, device='cuda:2')},\n",
       " {'running_loss': 0.23637283314019442,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.23228428699076176,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.22881654649972916,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.2258833609521389,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.22340665757656097,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.22131463419646025,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.219540199264884,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.21801944728940725,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.21669138222932816,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 0,\n",
       "  'test_acc': 0},\n",
       " {'running_loss': 0.21549667976796627,\n",
       "  'train_acc': tensor(100.0000, device='cuda:2'),\n",
       "  'test_running_loss': 56.57541039586067,\n",
       "  'test_acc': tensor(89.1039, device='cuda:2')}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e64d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:19:16.193265Z",
     "start_time": "2022-04-26T13:18:39.506967Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858a1dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:49.441174Z",
     "start_time": "2022-04-26T13:40:43.448546Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model/\" + model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce3380a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:49.450678Z",
     "start_time": "2022-04-26T13:40:49.444787Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "for result in results:\n",
    "    train_loss_list.append(result[\"running_loss\"])\n",
    "    train_acc_list.append(torch.Tensor.cpu(result[\"train_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa0f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:52.123664Z",
     "start_time": "2022-04-26T13:40:49.452164Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "for result in results:\n",
    "    test_loss_list.append(result[\"test_running_loss\"])\n",
    "    test_acc_list.append(torch.Tensor.cpu(result[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b20dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:52.127066Z",
     "start_time": "2022-04-26T13:40:52.127017Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "for result in results:\n",
    "    val_loss_list.append(result[\"val_loss\"])\n",
    "    val_acc_list.append(result[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eecb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T02:04:13.240546Z",
     "start_time": "2022-04-26T02:04:13.240502Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f96fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:40:52.130780Z",
     "start_time": "2022-04-26T13:40:52.130732Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "ax[0].plot(train_loss_list)\n",
    "ax[0].plot(test_loss_list)\n",
    "ax[0].legend(['train','test'])\n",
    "ax[0].grid()\n",
    "ax[0].set_title(\"Loss\")\n",
    "\n",
    "ax[1].plot(train_acc_list)\n",
    "ax[1].plot(test_acc_list)\n",
    "ax[1].legend(['train','test'])\n",
    "ax[1].grid()\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8602f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e68f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78385b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ccfde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
